{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b5e4fc-c495-4a8b-858b-b3cf1bf23f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pandas as pd\n",
    "import random\n",
    "import glob\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6e4f48-e2b4-46a3-b232-530f92b12d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spec_to_image(spec, eps=1e-6):\n",
    "    mean = spec.mean()\n",
    "    std = spec.std()\n",
    "    spec_norm = (spec - mean) / (std + eps)\n",
    "    spec_min, spec_max = spec_norm.min(), spec_norm.max()\n",
    "    spec_scaled = 255 * (spec_norm - spec_min) / (spec_max - spec_min)\n",
    "    spec_scaled = spec_scaled.astype(np.uint8)\n",
    "        \n",
    "    return spec_scaled\n",
    "\n",
    "def get_melspectrogram_db(file_path, aug=False, sr=48000, n_fft=2048, hop_length=256, n_mels=128, fmin=20, fmax=8300, top_db=80):\n",
    "    wav, _ = librosa.load(file_path, sr=sr)\n",
    "    \n",
    "    # # Ensure audio is at least 5 seconds\n",
    "    if wav.shape[0] < 3 * sr:\n",
    "        wav = np.pad(wav, int(np.ceil((3 * sr - wav.shape[0]) / 2)), mode='reflect')\n",
    "    else:\n",
    "        wav = wav[:3 * sr]\n",
    " \n",
    "    if aug:\n",
    "        if random.random() < 0.5:\n",
    "            wav = pitch_shift(wav, sr, n_steps=random.uniform(-1, 1))  # Pitch-shift\n",
    "    \n",
    "    spec = librosa.feature.melspectrogram(y=wav, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, fmin=fmin, fmax=fmax)\n",
    "    spec_db = librosa.power_to_db(spec, top_db=top_db)\n",
    "\n",
    "    return spec_db\n",
    "\n",
    "\n",
    "        \n",
    "# image_post = spec_to_image(get_melspectrogram_db(file_path, aug=self.augs))[np.newaxis, ...]\n",
    "# image_pre = spec_to_image(get_melspectrogram_db(file_path.replace('post', 'pre'), \n",
    "#                                                         aug=self.augs))[np.newaxis, ...]     \n",
    "# combined = np.concatenate([image_pre, image_post])\n",
    "# label = torch.tensor(self.c2i[row['class']], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c39e76-e70e-4dfb-abde-5be38bf09645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 97 total: 77 train, 10 val, 10 test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73801855-9034-42ac-80ce-1fd8cfdeed70",
   "metadata": {},
   "source": [
    "### Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a1ae0f-2e9e-4323-bdba-1d9953cf992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)\n",
    "\n",
    "# Data files\n",
    "data_path_pre = 'wetvoice/extractedMPTs/pre swallow/**'\n",
    "data_files_pre = [x for x in glob.glob(data_path_pre) if '.wav' in x and 'android' not in x]\n",
    "\n",
    "data_path_post = 'wetvoice/extractedMPTs/post swallow/**'\n",
    "data_files_post = [x for x in glob.glob(data_path_post) if '.wav' in x and 'android' not in x]\n",
    "\n",
    "ids = [x.split('/')[-1].split('_post')[0] for x in data_files_post]\n",
    "\n",
    "train_files = [x for x in data_files_pre if x.split('/')[-1].split('_pre')[0] in ids[:95]]\n",
    "# val_files = [x for x in data_files_pre if x.split('/')[-1].split('_pre')[0] in ids[77:87]]\n",
    "# test_files = [x for x in data_files_pre if x.split('/')[-1].split('_pre')[0] in ids[87:]]\n",
    "\n",
    "len(train_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241b6a15-4d73-44f4-8c66-457a079ba8c1",
   "metadata": {},
   "source": [
    "#### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02b0d8f-7698-4b06-b0f4-f2ef4be9c2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('audio file numbers and aspiration values.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb168b59-73d0-4f93-8fa1-45f7a72dcbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = [df[df['Audio File Name']==int(i.split('/')[-1].split('_pre')[0])]['Aspiration  '].values[0]\n",
    "                for i in train_files]\n",
    "train_labels = [1 if x == 'Yes' else 0 for x in train_labels]\n",
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e4c423-7a02-4ac7-ad20-c5dce107384f",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b803330-76e2-4cb5-a55f-35b4f10c0922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(f_path):\n",
    "    y, sr = librosa.load(f_path)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "    return mfcc.mean(axis=1)\n",
    "\n",
    "audio_paths_a = train_files\n",
    "audio_paths_b = [x.replace('post', 'pre') for x in train_files]\n",
    "\n",
    "features = []\n",
    "for path_a, path_b in zip(audio_paths_a, audio_paths_b):\n",
    "    features_a = extract_features(path_a)\n",
    "    features_b = extract_features(path_b)\n",
    "\n",
    "    feature_diff = np.abs(features_a - features_b)\n",
    "    features.append(feature_diff)\n",
    "\n",
    "features = np.array(features)\n",
    "labels = np.array(train_labels)  \n",
    "\n",
    "# Split the dataset \n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42,  stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593010db-85ca-4fef-8f16-8e4d5aa1c6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "knn.fit(X_train, y_train)\n",
    "predictions = knn.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a930865f-19ef-44bb-b697-22ae8da0d389",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = knn.predict(X_train)\n",
    "print(classification_report(y_train, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11166687-748b-483d-8e5d-c192fd13b92f",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753f339c-1150-4950-b42c-9ed92c8d696f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that includes scaling, PCA for dimensionality reduction, and KNN\n",
    "pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    PCA(n_components=3),  \n",
    "    KNeighborsClassifier(n_neighbors=5)\n",
    ")\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "predictions = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebe3ced-a6ca-4e4e-8492-f249f26a0864",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995fad53-4c11-4a76-aa71-2a8dba05086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "predictions = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e789ce-8baf-4f32-91b8-c5c2103ac4fa",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8294bdd-d275-41e9-9118-ccb357026e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "normal_features = features[labels == 0]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "normal_features_scaled = scaler.fit_transform(normal_features)\n",
    "\n",
    "oc_svm = OneClassSVM(kernel='rbf', gamma='auto')\n",
    "\n",
    "oc_svm.fit(normal_features_scaled)\n",
    "\n",
    "full_features_scaled = scaler.transform(features)\n",
    "predictions = oc_svm.predict(full_features_scaled)\n",
    "\n",
    "predictions = (predictions == -1).astype(int)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49cd204-98ee-49ad-b194-2f574eef6445",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
