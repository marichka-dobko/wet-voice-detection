{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b5e4fc-c495-4a8b-858b-b3cf1bf23f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pandas as pd\n",
    "import random\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from src.model import MobileNetv2Model, ResNet18Model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:1')\n",
    "else:\n",
    "    device = torch.device('cpu') \n",
    "    \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8383c009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import audiomentations\n",
    "from audiomentations import AddGaussianSNR, TimeStretch, PitchShift, Shift\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6e4f48-e2b4-46a3-b232-530f92b12d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spec_to_image(spec, eps=1e-6):\n",
    "    mean = spec.mean()\n",
    "    std = spec.std()\n",
    "    spec_norm = (spec - mean) / (std + eps)\n",
    "    spec_min, spec_max = spec_norm.min(), spec_norm.max()\n",
    "    spec_scaled = 255 * (spec_norm - spec_min) / (spec_max - spec_min)\n",
    "    spec_scaled = spec_scaled.astype(np.uint8)\n",
    "        \n",
    "    return spec_scaled\n",
    "\n",
    "def get_melspectrogram_db(file_path, aug=False, sr=48000, n_fft=2048, hop_length=256, n_mels=128, fmin=20, fmax=8300, top_db=80):\n",
    "    wav, _ = librosa.load(file_path, sr=sr)\n",
    "    \n",
    "    # # Ensure audio is at least 20 seconds\n",
    "    if wav.shape[0] < 14 * sr:\n",
    "        wav = np.pad(wav, int(np.ceil((14 * sr - wav.shape[0]) / 2)), mode='reflect')\n",
    "    else:\n",
    "        wav = wav[:14 * sr]\n",
    "    \n",
    "    audio_transforms = audiomentations.Compose([\n",
    "        AddGaussianSNR(min_snr_in_db=5, max_snr_in_db=40.0, p=0.5),\n",
    "        TimeStretch(min_rate=0.8, max_rate=1.25, p=0.5),\n",
    "        PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n",
    "        Shift(-0.5, 0.5, p=0.5),\n",
    "    ])\n",
    "    \n",
    "    if aug:\n",
    "        wav = audio_transforms(samples=wav, sample_rate=sr)\n",
    "     \n",
    "    spec = librosa.feature.melspectrogram(y=wav, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, fmin=fmin, fmax=fmax)\n",
    "    spec_db = librosa.power_to_db(spec, top_db=top_db)\n",
    "\n",
    "    return spec_db\n",
    "\n",
    "def array_to_tensor(img_array) -> torch.FloatTensor:\n",
    "    return torch.FloatTensor(img_array)\n",
    "\n",
    "class ImageCombinedDataset(Dataset):\n",
    "    def __init__(self, files, labels,  augs=False):\n",
    "        self.files = files\n",
    "        self.labels = labels\n",
    "        self.augs = augs\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_a = self.files[idx]\n",
    "        lbl = self.labels[idx]\n",
    "        file_b = file_a.replace('pre', 'post')\n",
    "                \n",
    "        image_post = spec_to_image(get_melspectrogram_db(file_b, aug=self.augs))[np.newaxis, ...]\n",
    "        image_pre = spec_to_image(get_melspectrogram_db(file_a, aug=self.augs))[np.newaxis, ...]\n",
    "        \n",
    "        combined = np.concatenate([image_pre, image_post])\n",
    "        \n",
    "        label = torch.tensor(lbl, dtype=torch.int64)\n",
    "        return array_to_tensor(combined), label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73801855-9034-42ac-80ce-1fd8cfdeed70",
   "metadata": {},
   "source": [
    "### Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a1ae0f-2e9e-4323-bdba-1d9953cf992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)\n",
    "\n",
    "# Data files\n",
    "data_path_pre = 'etvoice/extractedMPTs/pre swallow/**'\n",
    "data_files_pre = [x for x in glob.glob(data_path_pre) if '.wav' in x and 'android' not in x\n",
    "                 and 'apple' not in x]\n",
    "\n",
    "data_path_post = 'wetvoice/extractedMPTs/post swallow/**'\n",
    "data_files_post = [x for x in glob.glob(data_path_post) if '.wav' in x and 'android' not in x\n",
    "                  and 'apple' not in x]\n",
    "\n",
    "ids = [x.split('/')[-1].split('_post')[0] for x in data_files_post]\n",
    "\n",
    "train_files = [x for x in data_files_pre if x.split('/')[-1].split('_pre')[0] in ids]\n",
    "train_files.remove('wetvoice/extractedMPTs/pre swallow/159_pre_mpt.wav')\n",
    "\n",
    "# LABELS\n",
    "df = pd.read_excel('audio file numbers and aspiration values.xlsx')\n",
    "df.head()\n",
    "train_labels = [df[df['Audio File Name']==int(i.split('/')[-1].split('_pre')[0])]['Aspiration  '].values[0]\n",
    "                for i in train_files]\n",
    "train_labels = [1 if x == 'Yes' else 0 for x in train_labels]\n",
    "\n",
    "len(train_files),len(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6277936f",
   "metadata": {},
   "source": [
    "## K-fold split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e37eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of splits\n",
    "k = 5 \n",
    "kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "train_files = np.array(train_files)  \n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "folds = collections.defaultdict()\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(train_files, train_labels)):\n",
    "    folds[fold] = [train_files[train_index], train_files[test_index], \n",
    "                   train_labels[train_index], train_labels[test_index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8def5284",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e4a5d0-09b3-4152-baf4-4c389244912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fold: 0\n",
    "X_train, X_testing, y_train, y_testing = folds[0]\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_testing, y_testing,\n",
    "                                                    test_size=0.5, random_state=42,  stratify=y_testing)\n",
    "\n",
    "len(X_train), len(y_train), len(X_val), len(y_val), len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f740519e-1740-4965-a63f-f5ba54065eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ImageCombinedDataset(X_train, y_train, augs=True)\n",
    "valid_data = ImageCombinedDataset(X_val, y_val, augs=False)\n",
    "test_data = ImageCombinedDataset(X_test, y_test, augs=False)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=12, shuffle=True, num_workers=2) \n",
    "valid_loader = DataLoader(valid_data, batch_size=12, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_data, batch_size=12, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af86259-dbfe-4eab-aac2-c53da2ae9404",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9866d452-091b-4667-9b26-5a149084ae86",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# prop_class = (len(y_train) - np.sum(y_train)) / len(y_train)\n",
    "# class_weights = [prop_class, 1.]\n",
    "\n",
    "model = MobileNetv2Model(in_channels=2, num_classes=2)\n",
    "\n",
    "logger = TensorBoardLogger(save_dir=os.getcwd(), name=\"logs/mobilenet_augs_fold0\")\n",
    "\n",
    "trainer = pl.Trainer(logger=logger, accelerator='gpu', devices=[0],\n",
    "                     max_epochs=30, callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5,)],)\n",
    "\n",
    "model.hparams.lr = 1e-5  \n",
    "trainer.fit(model, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbad9339-56e5-4ef3-83bb-7a4e321df291",
   "metadata": {},
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83b933d-9cea-4896-8f7c-a50c9da1de1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = 'logs/mobilenet_augs_fold0/version_0/checkpoints/epoch=18-step=114.ckpt'\n",
    "model = MobileNetv2Model.load_from_checkpoint(check,  in_channels=2, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb63806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set \n",
    "model.eval()\n",
    "\n",
    "tr_data = ImageCombinedDataset(X_train, y_train, augs=False)\n",
    "results, labels = [], []    \n",
    "for i, sample in tqdm(enumerate(tr_data)): \n",
    "    img, cl = sample\n",
    "    labels.append(cl.item())    \n",
    "    logits = model(img.unsqueeze(0)).squeeze()  \n",
    "    res = torch.sigmoid(logits) > 0.5\n",
    "    results.append(res.item())\n",
    "\n",
    "print(confusion_matrix(labels, results))\n",
    "print('Precision', precision_score(labels, results, average='macro'))\n",
    "print('Recall', recall_score(labels, results, average='macro'))\n",
    "print('Accuracy', accuracy_score(labels, results))\n",
    "print('F1', f1_score(labels, results, average='macro'))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(labels, results).ravel()\n",
    "specificity = tn / (tn+fp)\n",
    "print(specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c13451a-8328-4021-9f88-1514922a84aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('\\n', 'Validation & Test')\n",
    "v_data = ImageCombinedDataset(X_testing, y_testing, augs=False)\n",
    "\n",
    "results, labels = [], []    \n",
    "for i, sample in tqdm(enumerate(v_data)): \n",
    "    img, cl = sample\n",
    "    labels.append(cl.item())\n",
    "    logits = model(img.unsqueeze(0)).squeeze()  \n",
    "    res = torch.sigmoid(logits) > 0.5\n",
    "    results.append(res.item())\n",
    "    \n",
    "print(confusion_matrix(labels, results))\n",
    "# Accuracy, Precision, Recall, F1-score, AUROC,\n",
    "print('Precision', precision_score(labels, results, average='macro'))\n",
    "print('Recall', recall_score(labels, results, average='macro'))\n",
    "print('Accuracy', accuracy_score(labels, results))\n",
    "print('F1', f1_score(labels, results, average='macro'))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(labels, results).ravel()\n",
    "specificity = tn / (tn+fp)\n",
    "print(specificity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
